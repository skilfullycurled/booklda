{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag \n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "from sets import Set\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "print \"imported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS SPECIFIC TO TOPIC MODELING\n",
    "\n",
    "import langid\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from configparser import ConfigParser\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "print \"imported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Documents and Splitting Into Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LOAD DOCUMENT BY LINE INTO AN ARRAY\n",
    "\n",
    "\n",
    "files = [filename.split('.')[0] for filename in os.listdir('digitalSTS/papers/txt/') if not filename.startswith('.')]\n",
    "# print files\n",
    "\n",
    "for filename in files:\n",
    "\n",
    "    inpath = 'digitalSTS/papers/txt/' + filename + '.txt'\n",
    "    outpath = 'digitalSTS/papers/lines/' + filename + '.csv'\n",
    "    \n",
    "    with open(inpath, 'r') as infile, open(outpath, 'w') as outfile:\n",
    "        \n",
    "#         for line in infile.readlines():\n",
    "#             print line\n",
    "        paragraphs = infile.readlines()\n",
    "    \n",
    "        for paragraph in paragraphs:\n",
    "\n",
    "            paragraph = paragraph.decode('unicode_escape').encode('ascii','ignore')\n",
    "            lines = [line.strip().lower() for line in ''.join(paragraph).split('.')]\n",
    "            #             print paragraph\n",
    "            \n",
    "            writer = csv.writer(outfile)\n",
    "\n",
    "            for line in lines:\n",
    "                words = line.split(' ')\n",
    "                if len(words) > 1:\n",
    "                    writer.writerow([line.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02_Parmiggiani_Monteiro', '03_Allhutter', '04_Camus_Vinck', '05_Lynch', '06_Vertesi', '07_Kerisadou', '08_Couture', '09_Dunbar_Hester', '10_Hawthorne', '11_Chan', '12_Poster', '13_Ilten_McInerney', '14_Nemer', '15_Erikson', '16_Toth', '17_Cohn', '18_Seaver', '19_Stark', '20_Ribes', '21_Cardoso_Llach', '22_Venturini', '23_Salamanca', '24_Munk', '25_Calvillo', '26_Winthereik', '27_Loukissas']\n"
     ]
    }
   ],
   "source": [
    "files = [filename.split('.')[0] for filename in os.listdir('digitalSTS/papers/csvs/') if not filename.startswith('.')]\n",
    "print files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionaries and Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(873 unique tokens: [u'represent', u'concept', u'managed', u'forbidden', u'lack']...)\n",
      "Dictionary(821 unique tokens: [u'represent', u'concept', u'semantic', u'grassroots', u'scratch']...)\n",
      "Dictionary(948 unique tokens: [u'concept', u'semantic', u'consider', u'magnetic', u'dynamic']...)\n",
      "Dictionary(784 unique tokens: [u'limited', u'code', u'forbidden', u'lack', u'focus']...)\n",
      "Dictionary(955 unique tokens: [u'represent', u'todays', u'consider', u'focus', u'yellow']...)\n",
      "Dictionary(782 unique tokens: [u'concept', u'managed', u'focus', u'four', u'woods']...)\n",
      "Dictionary(644 unique tokens: [u'limited', u'code', u'consider', u'chain', u'focus']...)\n",
      "Dictionary(698 unique tokens: [u'limited', u'code', u'illustrates', u'hooks', u'oss']...)\n",
      "Dictionary(772 unique tokens: [u'represent', u'concept', u'consider', u'limited', u'dissolution']...)\n",
      "Dictionary(794 unique tokens: [u'limited', u'code', u'managed', u'global', u'dynamic']...)\n",
      "Dictionary(1245 unique tokens: [u'represent', u'concept', u'partial', u'consider', u'breadth']...)\n",
      "Dictionary(711 unique tokens: [u'concept', u'lack', u'focus', u'existing', u'leads']...)\n",
      "Dictionary(580 unique tokens: [u'managed', u'lack', u'desirable', u'focus', u'steve']...)\n",
      "Dictionary(496 unique tokens: [u'concept', u'realty', u'dynamic', u'focus', u'manages']...)\n",
      "Dictionary(711 unique tokens: [u'limited', u'concept', u'particular', u'resistance', u'existing']...)\n",
      "Dictionary(783 unique tokens: [u'essay', u'represent', u'code', u'partial', u'consider']...)\n",
      "Dictionary(418 unique tokens: [u'limited', u'code', u'results', u'broader', u'issues']...)\n",
      "Dictionary(681 unique tokens: [u'represent', u'concept', u'managed', u'focus', u'four']...)\n",
      "Dictionary(840 unique tokens: [u'beckers', u'concept', u'consider', u'chain', u'similarity']...)\n",
      "Dictionary(612 unique tokens: [u'represent', u'code', u'legacies', u'varying', u'dynamic']...)\n",
      "Dictionary(639 unique tokens: [u'represent', u'consider', u'limited', u'lack', u'dynamic']...)\n",
      "Dictionary(522 unique tokens: [u'neighbors', u'represent', u'concept', u'limited', u'resistance']...)\n",
      "Dictionary(762 unique tokens: [u'represent', u'code', u'consider', u'assembles', u'focus']...)\n",
      "Dictionary(349 unique tokens: [u'represent', u'focus', u'decisions', u'epistemic', u'airscapes']...)\n",
      "Dictionary(526 unique tokens: [u'concept', u'consider', u'activated', u'focus', u'alien']...)\n",
      "Dictionary(596 unique tokens: [u'essay', u'managed', u'consider', u'curatorial', u'global']...)\n"
     ]
    }
   ],
   "source": [
    "files = [filename.split('.')[0] for filename in os.listdir('digitalSTS/papers/csvs/') if not filename.startswith('.')]\n",
    "# print files\n",
    "\n",
    "for filename in files:\n",
    "\n",
    "    inpath = 'digitalSTS/papers/csvs/' + filename + '.csv'\n",
    "    \n",
    "    with open(inpath, 'r') as infile:\n",
    "        \n",
    "        reader = csv.reader(infile)\n",
    "        documents = [line[0] for line in reader]\n",
    "#         print lines\n",
    "\n",
    "        # TOKENIZING\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        documents = [tokenizer.tokenize(doc.lower()) for doc in documents ]\n",
    "        \n",
    "        # GATHERING CANDIDATES FOR CUSTOM LIST OF STOPWORDS FROM NGRAMS\n",
    "        # ['i','m','a','a','i','t','6',...]\n",
    "        unigrams = [ w for doc in documents for w in doc if len(w)==1]\n",
    "        # ['do','or','or','of','be',...]\n",
    "        bigrams  = [ w for doc in documents for w in doc if len(w)==2]\n",
    "\n",
    "        misc = ['get', 'know', 'would']\n",
    "        \n",
    "        # CREATES THE SET OF UNIQUE \"CUSTOM STOP LIST WITH NLTK + THE ABOVE\n",
    "        stoplist  = set(nltk.corpus.stopwords.words(\"english\") + unigrams + bigrams + misc)\n",
    "\n",
    "        # REMOVES STOP WORDS\n",
    "        documents = [[token for token in doc if token not in stoplist]\n",
    "                        for doc in documents]\n",
    "\n",
    "        # REMOVES WORDS THAT ARE NUMBERS ONLY\n",
    "        documents = [ [token for token in doc if len(token.strip(digits)) == len(token)]\n",
    "                        for doc in documents ]\n",
    "        \n",
    "        # REMOVE WORDS THAT OCCUR ONLY ONCE\n",
    "        token_frequency = defaultdict(int)\n",
    "\n",
    "        # count all tokens\n",
    "        for doc in documents:\n",
    "            for token in doc:\n",
    "                token_frequency[token] += 1\n",
    "\n",
    "        # keep words that occur more than once\n",
    "        documents = [ [token for token in doc if token_frequency[token] > 1]\n",
    "                        for doc in documents  ]\n",
    "\n",
    "        # Sort words in documents\n",
    "        for doc in documents:\n",
    "            doc.sort()\n",
    "            \n",
    "        # I HATE UNICODE!!\n",
    "        documents = [ [token.decode('ascii', 'ignore') for token in doc] for doc in documents  ]\n",
    "        \n",
    "        # Build a dictionary where for each document each word has its own id\n",
    "        dictionary = corpora.Dictionary(documents)\n",
    "        dictionary.compactify()\n",
    "        # and save the dictionary for future use\n",
    "        print dictionary\n",
    "        dictionary.save( 'digitalSTS/papers/dicts/' + filename + '.dict')\n",
    "        \n",
    "        # BUILD AND SAVE CORPUS\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "        corpora.MmCorpus.serialize('digitalSTS/papers/corpuses/' + filename + '.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['digitized', 'coral', 'reefs'],\n",
       " ['elena', 'parmiggiani', 'and', 'eric', 'monteiro']]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE HELP FROM:\n",
    "# https://github.com/alexperrier/datatalks/blob/master/twitter/twitter_preprocessing.py\n",
    "\n",
    "# SPLITS EACH POST INTO WORDS WHILE REMOVING PUNCTUATION (\\w+) AND LOWER CASES THEM A WORD SUCH AS\n",
    "# I'M WILL BECOME I, M\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "documents = [tokenizer.tokenize(doc.lower()) for doc in documents ]\n",
    "## [['what', 'do', 'they', 'look', 'for', 'for', 'approval', 'or', 'denial'],\n",
    "##  ['hi','i','m','wondering','if','someone','could','answer',...]...\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a', 'a', 'a', 'a']\n",
      "['mr', 'in', 'is', 'in', 'on']\n"
     ]
    }
   ],
   "source": [
    "# I'M NOT SURE THESE ARE TECHNICALLY UNIGRAMS AND BIGRAMS SO MUCH AS THEY ARE SINGLE LETTER WORDS\n",
    "# AND TWO LETTER WORDS BUT WHO AM I TO JUDEGE (SHULDER SHURG)\n",
    "\n",
    "# ['i','m','a','a','i','t','6',...]\n",
    "unigrams = [ w for doc in documents for w in doc if len(w)==1]\n",
    "# ['do','or','or','of','be',...]\n",
    "bigrams  = [ w for doc in documents for w in doc if len(w)==2]\n",
    "\n",
    "misc = ['get', 'know', 'would']\n",
    "\n",
    "print unigrams[:5]\n",
    "print bigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['digitized', 'coral', 'reefs']\n",
      "['elena', 'parmiggiani', 'eric', 'monteiro']\n"
     ]
    }
   ],
   "source": [
    "# CREATES THE SET OF UNIQUE \"CUSTOM STOP LIST WITH NLTK + THE ABOVE\n",
    "stoplist  = set(nltk.corpus.stopwords.words(\"english\") + unigrams + bigrams + misc)\n",
    "\n",
    "# REMOVES STOP WORDS\n",
    "documents = [[token for token in doc if token not in stoplist]\n",
    "                for doc in documents]\n",
    "\n",
    "# REMOVES WORDS THAT ARE NUMBERS ONLY\n",
    "documents = [ [token for token in doc if len(token.strip(digits)) == len(token)]\n",
    "                for doc in documents ]\n",
    "\n",
    "# # [['look', 'approval', 'denial', 'parole', 'approval'...],\n",
    "# # ['wondering', 'someone', 'could', 'answer', 'question'...]...]\n",
    "print documents[0][:5]\n",
    "print documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coral', 'digitized', 'reefs']\n",
      "['monteiro', 'parmiggiani']\n"
     ]
    }
   ],
   "source": [
    "# REMOVE WORDS THAT OCCUR ONLY ONCE\n",
    "token_frequency = defaultdict(int)\n",
    "\n",
    "# count all tokens\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        token_frequency[token] += 1\n",
    "\n",
    "# keep words that occur more than once\n",
    "documents = [ [token for token in doc if token_frequency[token] > 1]\n",
    "                for doc in documents  ]\n",
    "\n",
    "# Sort words in documents\n",
    "for doc in documents:\n",
    "    doc.sort()\n",
    "    \n",
    "## [['approval', 'approval', 'approval', 'approval', 'approval'...],\n",
    "##  ['answer', 'benefit', 'benefit', 'benefit', 'confused'...]...]\n",
    "\n",
    "print documents[0][:5]\n",
    "print documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'coral', u'digitized', u'reefs']\n",
      "[u'monteiro', u'parmiggiani']\n"
     ]
    }
   ],
   "source": [
    "# I HATE UNICODE!!\n",
    "documents = [ [token.decode('ascii', 'ignore') for token in doc] for doc in documents  ]\n",
    "print documents[0][:5]\n",
    "print documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a dictionary where for each document each word has its own id\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.compactify()\n",
    "# and save the dictionary for future use\n",
    "dictionary.save('parmiggiani.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(873 unique tokens: [u'represent', u'concept', u'managed', u'forbidden', u'lack']...)\n"
     ]
    }
   ],
   "source": [
    "# We now have a dictionary with 11127 unique tokens\n",
    "# Dictionary(11127 unique tokens: [u'foul', u'narcotic', u'four', u'woods', u'hanging']...)\n",
    "print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the corpus: vectors with occurence of each word for each document\n",
    "# convert tokenized documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and save in Market Matrix format\n",
    "corpora.MmCorpus.serialize('parmiggiani.mm', corpus)\n",
    "# this corpus can be loaded with corpus = corpora.MmCorpus('prison_talk_nyc.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MISC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = [filename.split('.')[0] for filename in os.listdir('../papers/strings/') if not filename.startswith('.')]\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_Parmiggiani_Monteiro ----> TOKENS IN: 7519 POST STOP: 3991 POST HIGHFQ: 3136 ----> PCT: 42.0\n",
      "03_Allhutter ----> TOKENS IN: 4129 POST STOP: 2403 POST HIGHFQ: 1719 ----> PCT: 42.0\n",
      "04_Camus_Vinck ----> TOKENS IN: 10563 POST STOP: 5268 POST HIGHFQ: 4356 ----> PCT: 41.0\n",
      "05_Lynch ----> TOKENS IN: 7075 POST STOP: 3435 POST HIGHFQ: 2575 ----> PCT: 36.0\n",
      "06_Vertesi ----> TOKENS IN: 9048 POST STOP: 4755 POST HIGHFQ: 3811 ----> PCT: 42.0\n",
      "07_Kerisadou ----> TOKENS IN: 6299 POST STOP: 3201 POST HIGHFQ: 2387 ----> PCT: 38.0\n",
      "08_Couture ----> TOKENS IN: 7745 POST STOP: 3897 POST HIGHFQ: 3234 ----> PCT: 42.0\n",
      "09_Dunbar_Hester ----> TOKENS IN: 4436 POST STOP: 2392 POST HIGHFQ: 1541 ----> PCT: 35.0\n",
      "10_Hawthorne ----> TOKENS IN: 5563 POST STOP: 3151 POST HIGHFQ: 2241 ----> PCT: 40.0\n",
      "11_Chan ----> TOKENS IN: 5448 POST STOP: 3207 POST HIGHFQ: 2308 ----> PCT: 42.0\n",
      "12_Poster ----> TOKENS IN: 13320 POST STOP: 7193 POST HIGHFQ: 5922 ----> PCT: 44.0\n",
      "13_Ilten_McInerney ----> TOKENS IN: 4999 POST STOP: 2837 POST HIGHFQ: 2190 ----> PCT: 44.0\n",
      "14_Nemer ----> TOKENS IN: 6280 POST STOP: 3196 POST HIGHFQ: 2305 ----> PCT: 37.0\n",
      "15_Erikson ----> TOKENS IN: 3729 POST STOP: 1950 POST HIGHFQ: 1267 ----> PCT: 34.0\n",
      "16_Toth ----> TOKENS IN: 6391 POST STOP: 3275 POST HIGHFQ: 2386 ----> PCT: 37.0\n",
      "17_Cohn ----> TOKENS IN: 9739 POST STOP: 4740 POST HIGHFQ: 3826 ----> PCT: 39.0\n",
      "18_Seaver ----> TOKENS IN: 4267 POST STOP: 2193 POST HIGHFQ: 1490 ----> PCT: 35.0\n",
      "19_Stark ----> TOKENS IN: 4127 POST STOP: 2303 POST HIGHFQ: 1636 ----> PCT: 40.0\n",
      "20_Ribes ----> TOKENS IN: 7901 POST STOP: 3992 POST HIGHFQ: 3071 ----> PCT: 39.0\n",
      "21_Cardoso_Llach ----> TOKENS IN: 5738 POST STOP: 3307 POST HIGHFQ: 2573 ----> PCT: 45.0\n",
      "22_Venturini ----> TOKENS IN: 4988 POST STOP: 2514 POST HIGHFQ: 1742 ----> PCT: 35.0\n",
      "23_Salamanca ----> TOKENS IN: 4641 POST STOP: 2613 POST HIGHFQ: 2015 ----> PCT: 43.0\n",
      "24_Munk ----> TOKENS IN: 8021 POST STOP: 3995 POST HIGHFQ: 3125 ----> PCT: 39.0\n",
      "25_Calvillo ----> TOKENS IN: 2652 POST STOP: 1290 POST HIGHFQ: 834 ----> PCT: 31.0\n",
      "26_Winthereik ----> TOKENS IN: 5091 POST STOP: 2580 POST HIGHFQ: 1839 ----> PCT: 36.0\n",
      "27_Loukissas ----> TOKENS IN: 1812 POST STOP: 973 POST HIGHFQ: 538 ----> PCT: 30.0\n"
     ]
    }
   ],
   "source": [
    "# filename = '02_Parmiggiani_Monteiro'\n",
    "# testpath = '../papers/tests/' + '02_Parmiggiani_Monteiro' + '.csv'\n",
    "\n",
    "totalfq = collections.Counter()\n",
    "\n",
    "for filename in files:\n",
    "\n",
    "    inpath = '../papers/strings/' + filename + '.csv'\n",
    "    outpath = '../papers/lemmas/' + filename + '.csv'\n",
    "    \n",
    "    with open(inpath, 'r') as infile, open(outpath, 'w') as outfile:\n",
    "\n",
    "        essay = infile.readline()\n",
    "\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(essay.lower())\n",
    "\n",
    "        # ['i','m','a','a','i','t','6',...]\n",
    "        unichars = [word for word in tokens if len(word)==1]\n",
    "        # ['do','or','or','of','be',...]\n",
    "        bichars  =  [word for word in tokens if len(word)==2]\n",
    "\n",
    "        misc = ['get', 'know', 'would', 'and', 'yet', 'due', 'one', 'digital', 'technology', 'way', \\\n",
    "                'also', 'use', 'within', 'like', 'sts', 'may', 'however', 'even', u'technology', 'used']\n",
    "\n",
    "        stoplist  = set(nltk.corpus.stopwords.words(\"english\") + unichars + bichars + misc)\n",
    "\n",
    "        # REMOVES WORDS THAT ARE NUMBERS ONLY\n",
    "        post_stop = [token for token in tokens if len(token.strip(digits)) == len(token)]\n",
    "\n",
    "        # REMOVES STOP WORDS\n",
    "        post_stop = [token for token in tokens if token not in stoplist]\n",
    "\n",
    "        post_lemma = [lemma.lemmatize(token) for token in post_stop]\n",
    "\n",
    "        tokenfq = collections.Counter()\n",
    "        tokenfq.update(post_lemma)\n",
    "        \n",
    "        post_highfq = [token for token in post_lemma if tokenfq[token] > 1]\n",
    "\n",
    "        totalfq.update(post_highfq)\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow([' '.join(post_highfq).encode('utf-8')])\n",
    "\n",
    "        print filename, '----> TOKENS IN:', len(tokens), 'POST STOP:', len(post_stop), 'POST HIGHFQ:', len(post_highfq), \\\n",
    "                        '----> PCT:', round(float(len(post_highfq))/float(len(tokens)), 2) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
