{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag \n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "print \"imported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS SPECIFIC TO TOPIC MODELING\n",
    "\n",
    "import langid\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from configparser import ConfigParser\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from string import digits\n",
    "print \"imported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'digitalSTS/papers/txt/02_Parmiggiani_Monteiro.txt03_Allhutter.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-73505b1bcdbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'digitalSTS/papers/txt/02_Parmiggiani_Monteiro.txt03_Allhutter.txt'"
     ]
    }
   ],
   "source": [
    "files = [filename.split('.')[0] for filename in os.listdir(inpath) if not filename.startswith('.')]\n",
    "print files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Documents and Splitting Into Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LOAD DOCUMENT BY LINE INTO AN ARRAY\n",
    "\n",
    "\n",
    "files = [filename.split('.')[0] for filename in os.listdir('digitalSTS/papers/txt/') if not filename.startswith('.')]\n",
    "# print files\n",
    "\n",
    "for filename in files:\n",
    "\n",
    "    inpath = 'digitalSTS/papers/txt/' + filename + '.txt'\n",
    "    outpath = 'digitalSTS/papers/lines/' + filename + '.csv'\n",
    "    \n",
    "    with open(inpath, 'r') as infile, open(outpath, 'w') as outfile:\n",
    "        \n",
    "#         for line in infile.readlines():\n",
    "#             print line\n",
    "        paragraphs = infile.readlines()\n",
    "    \n",
    "        for paragraph in paragraphs:\n",
    "\n",
    "            paragraph = paragraph.decode('unicode_escape').encode('ascii','ignore')\n",
    "            lines = [line.strip().lower() for line in ''.join(paragraph).split('.')]\n",
    "            #             print paragraph\n",
    "            \n",
    "            writer = csv.writer(outfile)\n",
    "\n",
    "            for line in lines:\n",
    "                words = line.split(' ')\n",
    "                if len(words) > 1:\n",
    "                    writer.writerow([line.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02_Parmiggiani_Monteiro', '03_Allhutter', '04_Camus_Vinck', '05_Lynch', '06_Vertesi', '07_Kerisadou', '08_Couture', '09_Dunbar_Hester', '10_Hawthorne', '11_Chan', '12_Poster', '13_Ilten_McInerney', '14_Nemer', '15_Erikson', '16_Toth', '17_Cohn', '18_Seaver', '19_Stark', '20_Ribes', '21_Cardoso_Llach', '22_Venturini', '23_Salamanca', '24_Munk', '25_Calvillo', '26_Winthereik', '27_Loukissas']\n"
     ]
    }
   ],
   "source": [
    "files = [filename.split('.')[0] for filename in os.listdir('digitalSTS/papers/csvs/') if not filename.startswith('.')]\n",
    "print files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionaries and Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(873 unique tokens: [u'represent', u'concept', u'managed', u'forbidden', u'lack']...)\n",
      "Dictionary(821 unique tokens: [u'represent', u'concept', u'semantic', u'grassroots', u'scratch']...)\n",
      "Dictionary(948 unique tokens: [u'concept', u'semantic', u'consider', u'magnetic', u'dynamic']...)\n",
      "Dictionary(784 unique tokens: [u'limited', u'code', u'forbidden', u'lack', u'focus']...)\n",
      "Dictionary(955 unique tokens: [u'represent', u'todays', u'consider', u'focus', u'yellow']...)\n",
      "Dictionary(782 unique tokens: [u'concept', u'managed', u'focus', u'four', u'woods']...)\n",
      "Dictionary(644 unique tokens: [u'limited', u'code', u'consider', u'chain', u'focus']...)\n",
      "Dictionary(698 unique tokens: [u'limited', u'code', u'illustrates', u'hooks', u'oss']...)\n",
      "Dictionary(772 unique tokens: [u'represent', u'concept', u'consider', u'limited', u'dissolution']...)\n",
      "Dictionary(794 unique tokens: [u'limited', u'code', u'managed', u'global', u'dynamic']...)\n",
      "Dictionary(1245 unique tokens: [u'represent', u'concept', u'partial', u'consider', u'breadth']...)\n",
      "Dictionary(711 unique tokens: [u'concept', u'lack', u'focus', u'existing', u'leads']...)\n",
      "Dictionary(580 unique tokens: [u'managed', u'lack', u'desirable', u'focus', u'steve']...)\n",
      "Dictionary(496 unique tokens: [u'concept', u'realty', u'dynamic', u'focus', u'manages']...)\n",
      "Dictionary(711 unique tokens: [u'limited', u'concept', u'particular', u'resistance', u'existing']...)\n",
      "Dictionary(783 unique tokens: [u'essay', u'represent', u'code', u'partial', u'consider']...)\n",
      "Dictionary(418 unique tokens: [u'limited', u'code', u'results', u'broader', u'issues']...)\n",
      "Dictionary(681 unique tokens: [u'represent', u'concept', u'managed', u'focus', u'four']...)\n",
      "Dictionary(840 unique tokens: [u'beckers', u'concept', u'consider', u'chain', u'similarity']...)\n",
      "Dictionary(612 unique tokens: [u'represent', u'code', u'legacies', u'varying', u'dynamic']...)\n",
      "Dictionary(639 unique tokens: [u'represent', u'consider', u'limited', u'lack', u'dynamic']...)\n",
      "Dictionary(522 unique tokens: [u'neighbors', u'represent', u'concept', u'limited', u'resistance']...)\n",
      "Dictionary(762 unique tokens: [u'represent', u'code', u'consider', u'assembles', u'focus']...)\n",
      "Dictionary(349 unique tokens: [u'represent', u'focus', u'decisions', u'epistemic', u'airscapes']...)\n",
      "Dictionary(526 unique tokens: [u'concept', u'consider', u'activated', u'focus', u'alien']...)\n",
      "Dictionary(596 unique tokens: [u'essay', u'managed', u'consider', u'curatorial', u'global']...)\n"
     ]
    }
   ],
   "source": [
    "files = [filename.split('.')[0] for filename in os.listdir('digitalSTS/papers/csvs/') if not filename.startswith('.')]\n",
    "# print files\n",
    "\n",
    "for filename in files:\n",
    "\n",
    "    inpath = 'digitalSTS/papers/csvs/' + filename + '.csv'\n",
    "    \n",
    "    with open(inpath, 'r') as infile:\n",
    "        \n",
    "        reader = csv.reader(infile)\n",
    "        documents = [line[0] for line in reader]\n",
    "#         print lines\n",
    "\n",
    "        # TOKENIZING\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        documents = [tokenizer.tokenize(doc.lower()) for doc in documents ]\n",
    "        \n",
    "        # GATHERING CANDIDATES FOR CUSTOM LIST OF STOPWORDS FROM NGRAMS\n",
    "        # ['i','m','a','a','i','t','6',...]\n",
    "        unigrams = [ w for doc in documents for w in doc if len(w)==1]\n",
    "        # ['do','or','or','of','be',...]\n",
    "        bigrams  = [ w for doc in documents for w in doc if len(w)==2]\n",
    "\n",
    "        misc = ['get', 'know', 'would']\n",
    "        \n",
    "        # CREATES THE SET OF UNIQUE \"CUSTOM STOP LIST WITH NLTK + THE ABOVE\n",
    "        stoplist  = set(nltk.corpus.stopwords.words(\"english\") + unigrams + bigrams + misc)\n",
    "\n",
    "        # REMOVES STOP WORDS\n",
    "        documents = [[token for token in doc if token not in stoplist]\n",
    "                        for doc in documents]\n",
    "\n",
    "        # REMOVES WORDS THAT ARE NUMBERS ONLY\n",
    "        documents = [ [token for token in doc if len(token.strip(digits)) == len(token)]\n",
    "                        for doc in documents ]\n",
    "        \n",
    "        # REMOVE WORDS THAT OCCUR ONLY ONCE\n",
    "        token_frequency = defaultdict(int)\n",
    "\n",
    "        # count all tokens\n",
    "        for doc in documents:\n",
    "            for token in doc:\n",
    "                token_frequency[token] += 1\n",
    "\n",
    "        # keep words that occur more than once\n",
    "        documents = [ [token for token in doc if token_frequency[token] > 1]\n",
    "                        for doc in documents  ]\n",
    "\n",
    "        # Sort words in documents\n",
    "        for doc in documents:\n",
    "            doc.sort()\n",
    "            \n",
    "        # I HATE UNICODE!!\n",
    "        documents = [ [token.decode('ascii', 'ignore') for token in doc] for doc in documents  ]\n",
    "        \n",
    "        # Build a dictionary where for each document each word has its own id\n",
    "        dictionary = corpora.Dictionary(documents)\n",
    "        dictionary.compactify()\n",
    "        # and save the dictionary for future use\n",
    "        print dictionary\n",
    "        dictionary.save( 'digitalSTS/papers/dicts/' + filename + '.dict')\n",
    "        \n",
    "        # BUILD AND SAVE CORPUS\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "        corpora.MmCorpus.serialize('digitalSTS/papers/corpuses/' + filename + '.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['digitized', 'coral', 'reefs'],\n",
       " ['elena', 'parmiggiani', 'and', 'eric', 'monteiro']]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE HELP FROM:\n",
    "# https://github.com/alexperrier/datatalks/blob/master/twitter/twitter_preprocessing.py\n",
    "\n",
    "# SPLITS EACH POST INTO WORDS WHILE REMOVING PUNCTUATION (\\w+) AND LOWER CASES THEM A WORD SUCH AS\n",
    "# I'M WILL BECOME I, M\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "documents = [tokenizer.tokenize(doc.lower()) for doc in documents ]\n",
    "## [['what', 'do', 'they', 'look', 'for', 'for', 'approval', 'or', 'denial'],\n",
    "##  ['hi','i','m','wondering','if','someone','could','answer',...]...\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a', 'a', 'a', 'a']\n",
      "['mr', 'in', 'is', 'in', 'on']\n"
     ]
    }
   ],
   "source": [
    "# I'M NOT SURE THESE ARE TECHNICALLY UNIGRAMS AND BIGRAMS SO MUCH AS THEY ARE SINGLE LETTER WORDS\n",
    "# AND TWO LETTER WORDS BUT WHO AM I TO JUDEGE (SHULDER SHURG)\n",
    "\n",
    "# ['i','m','a','a','i','t','6',...]\n",
    "unigrams = [ w for doc in documents for w in doc if len(w)==1]\n",
    "# ['do','or','or','of','be',...]\n",
    "bigrams  = [ w for doc in documents for w in doc if len(w)==2]\n",
    "\n",
    "misc = ['get', 'know', 'would']\n",
    "\n",
    "print unigrams[:5]\n",
    "print bigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['digitized', 'coral', 'reefs']\n",
      "['elena', 'parmiggiani', 'eric', 'monteiro']\n"
     ]
    }
   ],
   "source": [
    "# CREATES THE SET OF UNIQUE \"CUSTOM STOP LIST WITH NLTK + THE ABOVE\n",
    "stoplist  = set(nltk.corpus.stopwords.words(\"english\") + unigrams + bigrams + misc)\n",
    "\n",
    "# REMOVES STOP WORDS\n",
    "documents = [[token for token in doc if token not in stoplist]\n",
    "                for doc in documents]\n",
    "\n",
    "# REMOVES WORDS THAT ARE NUMBERS ONLY\n",
    "documents = [ [token for token in doc if len(token.strip(digits)) == len(token)]\n",
    "                for doc in documents ]\n",
    "\n",
    "# # [['look', 'approval', 'denial', 'parole', 'approval'...],\n",
    "# # ['wondering', 'someone', 'could', 'answer', 'question'...]...]\n",
    "print documents[0][:5]\n",
    "print documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coral', 'digitized', 'reefs']\n",
      "['monteiro', 'parmiggiani']\n"
     ]
    }
   ],
   "source": [
    "# REMOVE WORDS THAT OCCUR ONLY ONCE\n",
    "token_frequency = defaultdict(int)\n",
    "\n",
    "# count all tokens\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        token_frequency[token] += 1\n",
    "\n",
    "# keep words that occur more than once\n",
    "documents = [ [token for token in doc if token_frequency[token] > 1]\n",
    "                for doc in documents  ]\n",
    "\n",
    "# Sort words in documents\n",
    "for doc in documents:\n",
    "    doc.sort()\n",
    "    \n",
    "## [['approval', 'approval', 'approval', 'approval', 'approval'...],\n",
    "##  ['answer', 'benefit', 'benefit', 'benefit', 'confused'...]...]\n",
    "\n",
    "print documents[0][:5]\n",
    "print documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'coral', u'digitized', u'reefs']\n",
      "[u'monteiro', u'parmiggiani']\n"
     ]
    }
   ],
   "source": [
    "# I HATE UNICODE!!\n",
    "documents = [ [token.decode('ascii', 'ignore') for token in doc] for doc in documents  ]\n",
    "print documents[0][:5]\n",
    "print documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a dictionary where for each document each word has its own id\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.compactify()\n",
    "# and save the dictionary for future use\n",
    "dictionary.save('parmiggiani.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(873 unique tokens: [u'represent', u'concept', u'managed', u'forbidden', u'lack']...)\n"
     ]
    }
   ],
   "source": [
    "# We now have a dictionary with 11127 unique tokens\n",
    "# Dictionary(11127 unique tokens: [u'foul', u'narcotic', u'four', u'woods', u'hanging']...)\n",
    "print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the corpus: vectors with occurence of each word for each document\n",
    "# convert tokenized documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and save in Market Matrix format\n",
    "corpora.MmCorpus.serialize('parmiggiani.mm', corpus)\n",
    "# this corpus can be loaded with corpus = corpora.MmCorpus('prison_talk_nyc.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
